import { config } from 'dotenv';
import { put } from '@vercel/blob';
import { readFile, writeFile, readdir } from 'fs/promises';
import { join } from 'path';

// Load environment variables
config({ path: '.env.local' });

const DATA_DIR = './public/data';
const BLOB_TOKEN = process.env.BLOB_READ_WRITE_TOKEN;

// Files that failed in the previous upload
const FAILED_FILES = [
  'bc_092g025_3_4_2_xyes_8_utm10_20170601_dsm.laz',
  'bc_dsm.copc.laz',
  'bc_092g025_3_4_3_xyes_8_utm10_20170601_dsm.laz',
  'bc_dsm_v12_west.laz',
  'bc_dsm_v12.laz'
];

async function uploadWithRetry(filename, filepath, maxRetries = 3) {
  console.log(`\n‚¨ÜÔ∏è  Uploading ${filename}...`);

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const fileContent = await readFile(filepath);
      const fileSizeMB = (fileContent.length / 1024 / 1024).toFixed(2);

      console.log(`   Attempt ${attempt}/${maxRetries} (${fileSizeMB} MB)`);

      const blob = await put(filename, fileContent, {
        access: 'public',
        token: BLOB_TOKEN,
      });

      console.log(`‚úÖ Uploaded: ${blob.url}`);
      return { filename, url: blob.url, success: true };
    } catch (error) {
      console.error(`‚ùå Attempt ${attempt} failed:`, error.message);

      if (attempt < maxRetries) {
        const waitTime = attempt * 5; // Wait longer between retries
        console.log(`   Waiting ${waitTime}s before retry...`);
        await new Promise(resolve => setTimeout(resolve, waitTime * 1000));
      }
    }
  }

  console.error(`üí• Failed after ${maxRetries} attempts`);
  return { filename, url: null, success: false };
}

async function retryFailedUploads() {
  console.log('üîÑ Retrying failed uploads...\n');
  console.log(`üì¶ Files to retry: ${FAILED_FILES.length}\n`);

  const results = [];

  // Upload one at a time (no parallelism for large files)
  for (const filename of FAILED_FILES) {
    const filepath = join(DATA_DIR, filename);
    const result = await uploadWithRetry(filename, filepath);
    results.push(result);
  }

  // Load existing blob-urls.json
  let existingUrls = {};
  try {
    const content = await readFile('./blob-urls.json', 'utf-8');
    existingUrls = JSON.parse(content);
  } catch (error) {
    console.error('‚ö†Ô∏è  Could not load existing blob-urls.json');
  }

  // Merge new uploads
  for (const result of results) {
    if (result.success) {
      existingUrls[result.filename] = result.url;
    }
  }

  // Save updated URLs
  await writeFile('./blob-urls.json', JSON.stringify(existingUrls, null, 2));
  console.log('\nüíæ Updated blob-urls.json');

  // Update JS config
  const configContent = `// Auto-generated by upload scripts
// Do not edit manually - run 'npm run upload-blob' to regenerate

export const BLOB_URLS = ${JSON.stringify(existingUrls, null, 2)};

// Helper function to get blob URL or fallback to local path (for dev)
export function getDataUrl(filename) {
  // In production, use Blob URL
  if (BLOB_URLS[filename]) {
    return BLOB_URLS[filename];
  }

  // In development, use local path
  return \`/data/\${filename}\`;
}
`;

  await writeFile('./src/config/blobUrls.js', configContent);
  console.log('üíæ Updated src/config/blobUrls.js');

  // Print summary
  const successful = results.filter(r => r.success).length;
  const failed = results.filter(r => !r.success).length;

  console.log('\nüìã Retry Summary:');
  console.log(`‚úÖ Successful: ${successful}`);
  console.log(`‚ùå Failed: ${failed}`);
  console.log(`üì¶ Total in Blob: ${Object.keys(existingUrls).length}/18`);

  if (failed > 0) {
    console.log('\n‚ö†Ô∏è  Some files still failed. These files may be too large.');
    console.log('   Consider using a smaller subset of files for your deployment.');
  } else {
    console.log('\nüéâ All files successfully uploaded to Vercel Blob!');
  }
}

retryFailedUploads().catch(console.error);
